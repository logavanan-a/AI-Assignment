{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d914fb-b515-4fe2-9593-916e2939ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499833e-9d89-4e96-9d5f-1d0e23564fa0",
   "metadata": {},
   "source": [
    "#### Question 1: Train a CartPole Agent\n",
    "\n",
    "Dataset Problem: Use the OpenAI Gym's CartPole-v1 environment to train an agent using a simple reinforcement learning algorithm.  Assume hyperparameters, as per requirement and develop the model. Try to apply the concepts discussed in class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8404f18a-ad7a-4247-8607-11e306964f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb642c3-cb20-4058-9e9d-f22c74e0dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n       \n",
    "n_states = env.observation_space.shape[0]  \n",
    "theta = np.random.rand(n_actions, n_states) * 0.01  \n",
    "alpha = 0.01       \n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b6bf01-2eb1-4dec-9cc4-bb8851b07465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26d7ef34-24a4-4ddd-8d59-708c2e06e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    probs = softmax(theta @ state)\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "    return action, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd89f0b-af6d-4d1a-a1fc-166424bc65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Return: 17.0\n",
      "Episode 200, Return: 47.0\n",
      "Episode 300, Return: 18.0\n",
      "Episode 400, Return: 11.0\n",
      "Episode 500, Return: 14.0\n",
      "Episode 600, Return: 28.0\n",
      "Episode 700, Return: 36.0\n",
      "Episode 800, Return: 16.0\n",
      "Episode 900, Return: 43.0\n",
      "Episode 1000, Return: 12.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1000):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    trajectory = []\n",
    "\n",
    "    while not done:\n",
    "        action, probs = choose_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        trajectory.append((state, action, reward, probs))\n",
    "        state = next_state\n",
    "\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for _, _, r, _ in reversed(trajectory):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    for (s, a, r, probs), Gt in zip(trajectory, returns):\n",
    "        grad_log = np.zeros_like(theta)\n",
    "        grad_log[a] = s - np.sum(probs[:, None] * s, axis=0)\n",
    "        theta += alpha * Gt * grad_log\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}, Return: {sum(r for _,_,r,_ in trajectory)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff220e4-8e86-4a62-b7c9-8463dcac6cc1",
   "metadata": {},
   "source": [
    "#### Question 2: Mountain Car with Q-Learning\n",
    "Dataset Problem: Use OpenAI Gym's MountainCar-v0 environment to train a Q-learning agent.\n",
    "Similar to the CartPole example, but with the Mountain Car environment. The Q-learning code will be similar, with adjustments to the state and action space to fit the Mountain Car environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c43ede9-0ca6-4421-ad6e-63026274da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e17fe8ae-1e31-47aa-b84f-a9b1572f66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (18, 14)  \n",
    "obs_low = env.observation_space.low\n",
    "obs_high = env.observation_space.high\n",
    "bins = [np.linspace(obs_low[i], obs_high[i], n_bins[i]) for i in range(len(n_bins))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b69a2ac-259e-4bd8-a624-321f8feca4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    return tuple(np.digitize(s, b) for s, b in zip(state, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db5a192a-b552-4214-af7b-f28be16c792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros(n_bins + (n_actions,))\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80fa5534-4f1c-4ff6-b137-893ec216a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward: -346.0\n",
      "Episode 200, Reward: -351.0\n",
      "Episode 300, Reward: -212.0\n",
      "Episode 400, Reward: -258.0\n",
      "Episode 500, Reward: -372.0\n",
      "Episode 600, Reward: -399.0\n",
      "Episode 700, Reward: -150.0\n",
      "Episode 800, Reward: -178.0\n",
      "Episode 900, Reward: -165.0\n",
      "Episode 1000, Reward: -225.0\n",
      "Episode 1100, Reward: -149.0\n",
      "Episode 1200, Reward: -197.0\n",
      "Episode 1300, Reward: -131.0\n",
      "Episode 1400, Reward: -167.0\n",
      "Episode 1500, Reward: -158.0\n",
      "Episode 1600, Reward: -156.0\n",
      "Episode 1700, Reward: -163.0\n",
      "Episode 1800, Reward: -151.0\n",
      "Episode 1900, Reward: -177.0\n",
      "Episode 2000, Reward: -159.0\n"
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    state = discretize_state(env.reset()[0])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(n_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_state_continuous, reward, done, truncated, info = env.step(action)\n",
    "        next_state = discretize_state(next_state_continuous)\n",
    "\n",
    "        best_next_action = np.max(Q[next_state])\n",
    "        Q[state][action] += alpha * (reward + gamma * best_next_action - Q[state][action])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        print(f\"Episode {ep+1}, Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a49ad7-4d7e-4067-ad43-ed3343dddffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
